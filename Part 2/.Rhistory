# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, carrier, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "Yes"
)
#create pipeline
pipe <- po("removeconstants") %>>%
po("imputemean") %>>%
po("encode", method = "treatment") %>>%
po("scale") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5))
learner <- lrn("classif.log_reg", predict_type = "prob")
graph <- pipe %>>% learner
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5))
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n") #text output to know where we are
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "Yes"
)
#create pipeline
pipe <- po("removeconstants") %>>%
po("imputemean") %>>%
po("encode", method = "treatment") %>>%
po("scale") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5))
learner <- lrn("classif.log_reg", predict_type = "prob")
graph <- pipe %>>% learner
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5))
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n") #text output to know where we are
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
pipe <- po("removeconstants") %>>%
po("imputemean") %>>%
po("encode", method = "treatment") %>>%
po("scale") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5))
learner <- lrn("classif.log_reg", predict_type = "prob")
graph <- pipe %>>% learner
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5))
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
library(ml3filters)
install.packages("mlr3filters")
library(ml3filters)
library(mlr3filters)
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n") #text output to know where we are
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
pipe <- po("removeconstants") %>>%
po("imputemean") %>>%
po("encode", method = "treatment") %>>%
po("scale") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5))
learner <- lrn("classif.log_reg", predict_type = "prob")
graph <- pipe %>>% learner
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5))
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
install.packages("FSelectorRcpp")
library(FSelectorRcpp)
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n") #text output to know where we are
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
pipe <- po("removeconstants") %>>%
po("imputemean") %>>%
po("encode", method = "treatment") %>>%
po("scale") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5))
learner <- lrn("classif.log_reg", predict_type = "prob")
graph <- pipe %>>% learner
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5))
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
task$feature_names
ask$nrow
task$nrow
for (yr in sort(unique(flight_data$year))) { #for every year in the "year" var (so repeat this for 2004, 2005, 2006, 2007 and 2008)
cat("Processing year:", yr, "\n") #text output to know where we are in the loop
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
graph <- po("filter", filter = flt("information_gain"),
param_vals = list(filter.frac = 0.5)) %>>% #keep only 50% of features ranked by information gain
lrn("classif.glmnet", predict_type = "prob") #used this cause too much memory
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5)) #cross validation w/ 5 folds
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
install.packages("glmnet")
for (yr in sort(unique(flight_data$year))) { #for every year in the "year" var (so repeat this for 2004, 2005, 2006, 2007 and 2008)
cat("Processing year:", yr, "\n") #text output to know where we are in the loop
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
graph <- po("filter", filter = flt("information_gain"),
param_vals = list(filter.frac = 0.5)) %>>% #keep only 50% of features ranked by information gain
lrn("classif.glmnet", predict_type = "prob") #used this cause too much memory
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5)) #cross validation w/ 5 folds
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
task$feature_types[task$feature_types$type == "character", ]
for (yr in sort(unique(flight_data$year))) { #for every year in the "year" var (so repeat this for 2004, 2005, 2006, 2007 and 2008)
cat("Processing year:", yr, "\n") #text output to know where we are in the loop
# filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin, dest, origin_lat, origin_lon, dest_lat, dest_lon
) %>%
filter(!is.na(diverted_bin))
#skip year if not enough data
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#create pipeline
graph <- po("encode", method = "one-hot") %>>%  # Converts character/factor to numeric dummies
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5)) %>>% #keep only 50% of features ranked by information gain
lrn("classif.glmnet", predict_type = "prob") #used this cause too much memory
graph_learner <- GraphLearner$new(graph)
#resample
set.seed(123)
rr <- resample(task, graph_learner, rsmp("cv", folds = 5)) #cross validation w/ 5 folds
#store the AUC
auc <- rr$aggregate(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#retrain on full data to extract features and coefficients
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#get the coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
}
flight_data_raw <- ontime_db %>%
select(
year, month, dayofweek,
origin, dest,
crsdeptime, crsarrtime,
distance, diverted, uniquecarrier
) %>%
filter(!is.na(diverted)) %>% #remove missing diverted
collect()
flight_data <- flight_data_raw %>%
mutate(
diverted_bin = ifelse(diverted == 1, 1, 0), #make it ones and zeros, in case something is wrong it needs to be bin
sched_dep_hour = floor(crsdeptime / 100), #format of hour
sched_arr_hour = floor(crsarrtime / 100)
) %>%
select(-diverted, -crsdeptime, -crsarrtime)
airports <- airports_db %>%
collect()
#rename before joing to make sure everything is correct and ready to join dependidng on arrival and destination
airports_origin <- airports %>%
select(origin = iata, origin_lat = lat, origin_lon = long)
airports_dest <- airports %>%
select(dest = iata, dest_lat = lat, dest_lon = long)
# Merge both
flight_data <- flight_data %>%
left_join(airports_origin, by = "origin") %>%
left_join(airports_dest, by = "dest")
flight_data <- flight_data %>%
mutate(diverted_bin = factor(diverted_bin, levels = c(0, 1), ))
rm(flight_data_raw)
rm(airports_dest)
rm(airports_origin)
rm(flight_age_delay)
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n")
#filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin_lat, origin_lon, dest_lat, dest_lon, uniquecarrier
) %>%
filter(!is.na(diverted_bin))
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#pipeline: encode - filter -logistic regression
graph <- po("encode", method = "one-hot") %>>%
po("filter", filter = flt("information_gain"),
param_vals = list(filter.frac = 0.5)) %>>%
lrn("classif.log_reg", predict_type = "prob")
graph_learner <- GraphLearner$new(graph)
#split: 70% training and 30% test
set.seed(123)
split <- partition(task, ratio = 0.7)
#train and predict
graph_learner$train(task, row_ids = split$train)
prediction <- graph_learner$predict(task, row_ids = split$test)
#store AUC
auc <- prediction$score(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#get coefficients and selected features
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#extract coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
#cleanup because of ram
gc()
}
for (yr in sort(unique(flight_data$year))) {
cat("Processing year:", yr, "\n")
#filter the data of the selected year
df <- flight_data %>%
filter(year == yr) %>%
select(
diverted_bin, month, dayofweek, sched_dep_hour, sched_arr_hour,
distance, origin_lat, origin_lon, dest_lat, dest_lon, uniquecarrier
) %>%
filter(!is.na(diverted_bin))
if (nrow(df) < 1000 || length(unique(df$diverted_bin)) < 2) {
cat("Skipping year", yr, "- insufficient data\n")
next
}
#create task
task <- TaskClassif$new(
id = paste0("diverted_", yr),
backend = df,
target = "diverted_bin",
positive = "1"
)
#pipeline: encode - filter -logistic regression
graph <- po("imputemean") %>>%
po("encode", method = "one-hot") %>>%
po("filter", filter = flt("information_gain"), param_vals = list(filter.frac = 0.5)) %>>%
lrn("classif.log_reg", predict_type = "prob")
graph_learner <- GraphLearner$new(graph)
#split: 70% training and 30% test
set.seed(123)
split <- partition(task, ratio = 0.7)
#train and predict
graph_learner$train(task, row_ids = split$train)
prediction <- graph_learner$predict(task, row_ids = split$test)
#store AUC
auc <- prediction$score(msr("classif.auc"))
results[[as.character(yr)]] <- auc
#get coefficients and selected features
graph_learner$train(task)
selected_feats <- graph_learner$pipeops$filter$state$features
features_by_year[[as.character(yr)]] <- selected_feats
#extract coefficients
coefs <- as.data.frame(graph_learner$model$learner$model$coefficients)
colnames(coefs) <- "coefficient"
coefs$feature <- rownames(coefs)
coefs$year <- yr
coefs_by_year[[as.character(yr)]] <- coefs
#cleanup because of ram
gc()
}
